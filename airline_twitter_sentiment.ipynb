{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets=pd.read_csv('Tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14635</th>\n",
       "      <td>569587686496825344</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3487</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>KristenReenders</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir thank you we got on a different f...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 12:01:01 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14636</th>\n",
       "      <td>569587371693355008</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>itsropes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:59:46 -0800</td>\n",
       "      <td>Texas</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14637</th>\n",
       "      <td>569587242672398336</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sanyabun</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir Please bring American Airlines to...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:59:15 -0800</td>\n",
       "      <td>Nigeria,lagos</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14638</th>\n",
       "      <td>569587188687634433</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>0.6659</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SraJackson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir you have my money, you change my ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:59:02 -0800</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14639</th>\n",
       "      <td>569587140490866689</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6771</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>daviddtwu</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir we have 8 ppl so we need 2 know h...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 11:58:51 -0800</td>\n",
       "      <td>dallas, TX</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "14635  569587686496825344          positive                        0.3487   \n",
       "14636  569587371693355008          negative                        1.0000   \n",
       "14637  569587242672398336           neutral                        1.0000   \n",
       "14638  569587188687634433          negative                        1.0000   \n",
       "14639  569587140490866689           neutral                        0.6771   \n",
       "\n",
       "               negativereason  negativereason_confidence   airline  \\\n",
       "14635                     NaN                     0.0000  American   \n",
       "14636  Customer Service Issue                     1.0000  American   \n",
       "14637                     NaN                        NaN  American   \n",
       "14638  Customer Service Issue                     0.6659  American   \n",
       "14639                     NaN                     0.0000  American   \n",
       "\n",
       "      airline_sentiment_gold             name negativereason_gold  \\\n",
       "14635                    NaN  KristenReenders                 NaN   \n",
       "14636                    NaN         itsropes                 NaN   \n",
       "14637                    NaN         sanyabun                 NaN   \n",
       "14638                    NaN       SraJackson                 NaN   \n",
       "14639                    NaN        daviddtwu                 NaN   \n",
       "\n",
       "       retweet_count                                               text  \\\n",
       "14635              0  @AmericanAir thank you we got on a different f...   \n",
       "14636              0  @AmericanAir leaving over 20 minutes Late Flig...   \n",
       "14637              0  @AmericanAir Please bring American Airlines to...   \n",
       "14638              0  @AmericanAir you have my money, you change my ...   \n",
       "14639              0  @AmericanAir we have 8 ppl so we need 2 know h...   \n",
       "\n",
       "      tweet_coord              tweet_created tweet_location  \\\n",
       "14635         NaN  2015-02-22 12:01:01 -0800            NaN   \n",
       "14636         NaN  2015-02-22 11:59:46 -0800          Texas   \n",
       "14637         NaN  2015-02-22 11:59:15 -0800  Nigeria,lagos   \n",
       "14638         NaN  2015-02-22 11:59:02 -0800     New Jersey   \n",
       "14639         NaN  2015-02-22 11:58:51 -0800     dallas, TX   \n",
       "\n",
       "                    user_timezone  \n",
       "14635                         NaN  \n",
       "14636                         NaN  \n",
       "14637                         NaN  \n",
       "14638  Eastern Time (US & Canada)  \n",
       "14639                         NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_data=tweets.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_label=tweets.airline_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@VirginAmerica thanks to your outstanding NYC-JFK crew who moved mountains to get me home to San Francisco tonight!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_data[123]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuy\\AppData\\Local\\Continuum\\Anaconda3\\envs\\py34\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "example1=BeautifulSoup(text_data[123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@VirginAmerica thanks to your outstanding NYC-JFK crew who moved mountains to get me home to San Francisco tonight!'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example1.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "plain_text=[]\n",
    "for i in text_data:\n",
    "    letters_only = re.sub(\"[^a-zA-Z-#]\",           # The pattern to search for\n",
    "                      \" \",                   # The pattern to replace it with\n",
    "                      i)# The text to search\n",
    "    lower_case=letters_only.lower()\n",
    "    word=lower_case.split()\n",
    "    plain_text.append(word)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'which', u'who', u'whom', u'this', u'that', u'these', u'those', u'am', u'is', u'are', u'was', u'were', u'be', u'been', u'being', u'have', u'has', u'had', u'having', u'do', u'does', u'did', u'doing', u'a', u'an', u'the', u'and', u'but', u'if', u'or', u'because', u'as', u'until', u'while', u'of', u'at', u'by', u'for', u'with', u'about', u'against', u'between', u'into', u'through', u'during', u'before', u'after', u'above', u'below', u'to', u'from', u'up', u'down', u'in', u'out', u'on', u'off', u'over', u'under', u'again', u'further', u'then', u'once', u'here', u'there', u'when', u'where', u'why', u'how', u'all', u'any', u'both', u'each', u'few', u'more', u'most', u'other', u'some', u'such', u'no', u'nor', u'not', u'only', u'own', u'same', u'so', u'than', u'too', u'very', u's', u't', u'can', u'will', u'just', u'don', u'should', u'now', u'd', u'll', u'm', u'o', u're', u've', u'y', u'ain', u'aren', u'couldn', u'didn', u'doesn', u'hadn', u'hasn', u'haven', u'isn', u'ma', u'mightn', u'mustn', u'needn', u'shan', u'shouldn', u'wasn', u'weren', u'won', u'wouldn']\n"
     ]
    }
   ],
   "source": [
    "#check to see if nltk is imported\n",
    "from nltk.corpus import stopwords\n",
    "print (stopwords.words(\"english\") )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = [w for w in plain_text if not w in stopwords.words(\"english\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['virginamerica',\n",
       " 'plz',\n",
       " 'help',\n",
       " 'me',\n",
       " 'win',\n",
       " 'my',\n",
       " 'bid',\n",
       " 'upgrade',\n",
       " 'for',\n",
       " 'my',\n",
       " 'flight',\n",
       " 'lax---',\n",
       " 'gt',\n",
       " 'sea']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words[42]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14640"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set values for various parameters\n",
    "num_features =10    # Word vector dimensionality                      \n",
    "min_word_count = 20   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print \"Training model...\"\n",
    "model = word2vec.Word2Vec(words, workers=num_workers, \n",
    "                          size=num_features,\n",
    "                          min_count = min_word_count,\n",
    "                          window = context, sample = downsampling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = \"twitter_airline\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ticket'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"ticket thank bad awful\".split()) \n",
    "#test to see if it has train the model well enough to recognize dissimiliarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('im', 0.9956424236297607),\n",
       " ('plan', 0.9951696395874023),\n",
       " ('planned', 0.9941737055778503),\n",
       " ('#usairwaysfail', 0.9927068948745728),\n",
       " ('per', 0.9883135557174683),\n",
       " ('mine', 0.9882388114929199),\n",
       " ('myself', 0.9879583716392517),\n",
       " ('traveler', 0.9873925447463989),\n",
       " ('beyond', 0.9860208034515381),\n",
       " ('overnight', 0.9857382774353027)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"upset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.25387949,  0.72280163, -0.24582884, -0.37226173, -0.41969997,\n",
       "       -0.0613648 , -0.02013431, -0.37177032,  0.07630681,  0.60755378], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['upset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np  # Make sure that numpy is imported\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #\n",
    "    nwords = 0\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set =set(model.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    # \n",
    "    # Loop through each reviews\n",
    "    for review in reviews:\n",
    "         # Print a status message every 1000th review\n",
    "        if counter%100. == 0.:\n",
    "            print \"Review %d of %d\" % (counter, len(reviews))\n",
    "       \n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "            reviewFeatureVecs[counter] = makeFeatureVec(review, model,num_features)\n",
    "     \n",
    "        counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 14640\n",
      "Review 100 of 14640\n",
      "Review 200 of 14640\n",
      "Review 300 of 14640\n",
      "Review 400 of 14640\n",
      "Review 500 of 14640\n",
      "Review 600 of 14640\n",
      "Review 700 of 14640\n",
      "Review 800 of 14640\n",
      "Review 900 of 14640\n",
      "Review 1000 of 14640\n",
      "Review 1100 of 14640\n",
      "Review 1200 of 14640\n",
      "Review 1300 of 14640\n",
      "Review 1400 of 14640\n",
      "Review 1500 of 14640\n",
      "Review 1600 of 14640\n",
      "Review 1700 of 14640\n",
      "Review 1800 of 14640\n",
      "Review 1900 of 14640\n",
      "Review 2000 of 14640\n",
      "Review 2100 of 14640\n",
      "Review 2200 of 14640\n",
      "Review 2300 of 14640\n",
      "Review 2400 of 14640\n",
      "Review 2500 of 14640\n",
      "Review 2600 of 14640\n",
      "Review 2700 of 14640\n",
      "Review 2800 of 14640\n",
      "Review 2900 of 14640\n",
      "Review 3000 of 14640\n",
      "Review 3100 of 14640\n",
      "Review 3200 of 14640\n",
      "Review 3300 of 14640\n",
      "Review 3400 of 14640\n",
      "Review 3500 of 14640\n",
      "Review 3600 of 14640\n",
      "Review 3700 of 14640\n",
      "Review 3800 of 14640\n",
      "Review 3900 of 14640\n",
      "Review 4000 of 14640\n",
      "Review 4100 of 14640\n",
      "Review 4200 of 14640\n",
      "Review 4300 of 14640\n",
      "Review 4400 of 14640\n",
      "Review 4500 of 14640\n",
      "Review 4600 of 14640\n",
      "Review 4700 of 14640\n",
      "Review 4800 of 14640\n",
      "Review 4900 of 14640\n",
      "Review 5000 of 14640\n",
      "Review 5100 of 14640\n",
      "Review 5200 of 14640\n",
      "Review 5300 of 14640\n",
      "Review 5400 of 14640\n",
      "Review 5500 of 14640\n",
      "Review 5600 of 14640\n",
      "Review 5700 of 14640\n",
      "Review 5800 of 14640\n",
      "Review 5900 of 14640\n",
      "Review 6000 of 14640\n",
      "Review 6100 of 14640\n",
      "Review 6200 of 14640\n",
      "Review 6300 of 14640\n",
      "Review 6400 of 14640\n",
      "Review 6500 of 14640\n",
      "Review 6600 of 14640\n",
      "Review 6700 of 14640\n",
      "Review 6800 of 14640\n",
      "Review 6900 of 14640\n",
      "Review 7000 of 14640\n",
      "Review 7100 of 14640\n",
      "Review 7200 of 14640\n",
      "Review 7300 of 14640\n",
      "Review 7400 of 14640\n",
      "Review 7500 of 14640\n",
      "Review 7600 of 14640\n",
      "Review 7700 of 14640\n",
      "Review 7800 of 14640\n",
      "Review 7900 of 14640\n",
      "Review 8000 of 14640\n",
      "Review 8100 of 14640\n",
      "Review 8200 of 14640\n",
      "Review 8300 of 14640\n",
      "Review 8400 of 14640\n",
      "Review 8500 of 14640\n",
      "Review 8600 of 14640\n",
      "Review 8700 of 14640\n",
      "Review 8800 of 14640\n",
      "Review 8900 of 14640\n",
      "Review 9000 of 14640\n",
      "Review 9100 of 14640\n",
      "Review 9200 of 14640\n",
      "Review 9300 of 14640\n",
      "Review 9400 of 14640\n",
      "Review 9500 of 14640\n",
      "Review 9600 of 14640\n",
      "Review 9700 of 14640\n",
      "Review 9800 of 14640\n",
      "Review 9900 of 14640\n",
      "Review 10000 of 14640\n",
      "Review 10100 of 14640\n",
      "Review 10200 of 14640\n",
      "Review 10300 of 14640\n",
      "Review 10400 of 14640\n",
      "Review 10500 of 14640\n",
      "Review 10600 of 14640\n",
      "Review 10700 of 14640\n",
      "Review 10800 of 14640\n",
      "Review 10900 of 14640\n",
      "Review 11000 of 14640\n",
      "Review 11100 of 14640\n",
      "Review 11200 of 14640\n",
      "Review 11300 of 14640\n",
      "Review 11400 of 14640\n",
      "Review 11500 of 14640\n",
      "Review 11600 of 14640\n",
      "Review 11700 of 14640\n",
      "Review 11800 of 14640\n",
      "Review 11900 of 14640\n",
      "Review 12000 of 14640\n",
      "Review 12100 of 14640\n",
      "Review 12200 of 14640\n",
      "Review 12300 of 14640\n",
      "Review 12400 of 14640\n",
      "Review 12500 of 14640\n",
      "Review 12600 of 14640\n",
      "Review 12700 of 14640\n",
      "Review 12800 of 14640\n",
      "Review 12900 of 14640\n",
      "Review 13000 of 14640\n",
      "Review 13100 of 14640\n",
      "Review 13200 of 14640\n",
      "Review 13300 of 14640\n",
      "Review 13400 of 14640\n",
      "Review 13500 of 14640\n",
      "Review 13600 of 14640\n",
      "Review 13700 of 14640\n",
      "Review 13800 of 14640\n",
      "Review 13900 of 14640\n",
      "Review 14000 of 14640\n",
      "Review 14100 of 14640\n",
      "Review 14200 of 14640\n",
      "Review 14300 of 14640\n",
      "Review 14400 of 14640\n",
      "Review 14500 of 14640\n",
      "Review 14600 of 14640\n"
     ]
    }
   ],
   "source": [
    "trainDataVecs=getAvgFeatureVecs(text_data, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataVecs[3429]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier( n_estimators = 100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting a random forest to labeled training data...\n"
     ]
    }
   ],
   "source": [
    "print \"Fitting a random forest to labeled training data...\"\n",
    "forest = forest.fit( trainDataVecs, text_label )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuy\\AppData\\Local\\Continuum\\Anaconda3\\envs\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['negative'], dtype=object)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = forest.predict( trainDataVecs[6232] )\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "not enough data/words in the vocabulary to use word2vec effectivetly. Next, try bag of word with some sklearn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_vectorizer(X):\n",
    "    #model = MultinomialNB() #Multinomial better than Bernoulli\n",
    "    model = SGDClassifier(loss='log', penalty='l2',alpha=1e-4, n_iter=10, random_state=42,shuffle=True)\n",
    "    #SGDClassifier()\n",
    "\n",
    "   \n",
    "\n",
    "    #X = X_train_counts\n",
    "    y = text_label\n",
    "\n",
    "    from sklearn.cross_validation import StratifiedKFold\n",
    "    skf = StratifiedKFold(y, n_folds=5)\n",
    "\n",
    "    ypred = np.zeros_like(y)\n",
    "    fold=0\n",
    "    for train_index, test_index in skf:\n",
    "        fold+=1\n",
    "        print (fold)\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        model.fit(X_train,y_train)\n",
    "        ypred[test_index]=model.predict(X_test)\n",
    "    return np.mean(y==ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.71536885245901638"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vectorizer(x_train_counts) #bi-gram gave the best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.71250000000000002"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vectorizer(x_train_counts) # uni-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "airline=pd.get_dummies(tweets.airline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>American</th>\n",
       "      <th>Delta</th>\n",
       "      <th>Southwest</th>\n",
       "      <th>US Airways</th>\n",
       "      <th>United</th>\n",
       "      <th>Virgin America</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   American  Delta  Southwest  US Airways  United  Virgin America\n",
       "0       0.0    0.0        0.0         0.0     0.0             1.0\n",
       "1       0.0    0.0        0.0         0.0     0.0             1.0\n",
       "2       0.0    0.0        0.0         0.0     0.0             1.0\n",
       "3       0.0    0.0        0.0         0.0     0.0             1.0\n",
       "4       0.0    0.0        0.0         0.0     0.0             1.0"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#To convert a Series or list-like object of date-like objects e.g. strings, epochs,\n",
    "#or a mixture, you can use the to_datetime function.\n",
    "timestamp=pd.to_datetime(tweets.tweet_created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "time=pd.DataFrame({'day': timestamp.dt.dayofweek})\n",
    "                    \n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   day\n",
       "0    1\n",
       "1    1\n",
       "2    1\n",
       "3    1\n",
       "4    1"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "airline_time=pd.concat([airline,time], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>American</th>\n",
       "      <th>Delta</th>\n",
       "      <th>Southwest</th>\n",
       "      <th>US Airways</th>\n",
       "      <th>United</th>\n",
       "      <th>Virgin America</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   American  Delta  Southwest  US Airways  United  Virgin America  day\n",
       "0       0.0    0.0        0.0         0.0     0.0             1.0    1\n",
       "1       0.0    0.0        0.0         0.0     0.0             1.0    1\n",
       "2       0.0    0.0        0.0         0.0     0.0             1.0    1\n",
       "3       0.0    0.0        0.0         0.0     0.0             1.0    1\n",
       "4       0.0    0.0        0.0         0.0     0.0             1.0    1"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airline_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_data=hstack((airline, x_train_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.coo.coo_matrix"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_data) #this is in coo matrix, convert to csr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages of the CSC format\n",
    "\n",
    "        efficient arithmetic operations CSC + CSC, CSC * CSC, etc.\n",
    "        efficient column slicing\n",
    "        fast matrix vector products (CSR, BSR may be faster)\n",
    "\n",
    "Disadvantages of the CSC format\n",
    "\n",
    "        slow row slicing operations (consider CSR)\n",
    "        changes to the sparsity structure are expensive (consider LIL or DOK)\n",
    "\n",
    "\n",
    "\n",
    "Advantages of the COO format\n",
    "\n",
    "        facilitates fast conversion among sparse formats\n",
    "        permits duplicate entries (see example)\n",
    "        very fast conversion to and from CSR/CSC formats\n",
    "\n",
    "Disadvantages of the COO format\n",
    "\n",
    "        does not directly support:\n",
    "                arithmetic operations\n",
    "                slicing\n",
    "\n",
    "Intended Usage\n",
    "\n",
    "        COO is a fast format for constructing sparse matrices\n",
    "        Once a matrix has been constructed, convert to CSR or CSC format for fast arithmetic and matrix vector operations\n",
    "        By default when converting to CSR or CSC format, duplicate (i,j) entries will be summed together. This facilitates efficient construction of finite element matrices and the like. (see example)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "x_train_data =csr_matrix(X_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.71830601092896174"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vectorizer(x_train_data) #doesn't include the date and timestamp, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.71413934426229508"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_vectorizer(x_train_data) #includes day of week, SGDclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.67663934426229511"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#multinomial\n",
    "test_vectorizer(x_train_data)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py27]",
   "language": "python",
   "name": "conda-env-py27-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
